---
title: "tiktokipv"
output: pdf_document
---

```{r setup, include=T}
library(tidytext)

get_sentiments("nrc")
```


```{r}

library(dplyr)
library(stringr)


mastersheet$Comment %>%
                        str_remove_all(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)") %>%
                        str_replace_all("&amp;", "and") %>%
                        str_remove_all("[[:punct:]]") %>%
                        str_remove_all("@") %>%
                        str_remove_all("\\d+\\w*\\d*") %>%
                        str_remove_all("#[[:alnum:]]+") %>%
                        str_replace_all("\\\n", " ") %>%
                        str_to_lower() %>%
                        str_trim("both")
 
   

 

```


remove emojis
```{r }
mastersheet$Comment <- sapply(mastersheet$Comment,function(row) iconv(row, "latin1", "ASCII", sub=))
```

remove blank rows


```{r}
mastersheet <-  mastersheet[!(is.na(mastersheet$Comment) | mastersheet$Comment==""), ]
```


load excel mastersheet
```{r}

mastersheet_clean <- tibble(platform = mastersheet$Platform,
                  comment = mastersheet$Comment,
                  likes = mastersheet$Likes)

```

```{r}
cleandata2 <- mastersheet_clean %>%
        unnest_tokens(word, comment)
```


```{r}
cleandata3 %>%
        count(word, sort = TRUE)
```
```{r}
      
```
```{r}
count(word, sort = TRUE)

mastersheet_clean$comment %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```
```{r}
get_sentiments("nrc") %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
  count(sentiment)
```
```{r}
get_sentiments("bing") %>% 
  count(sentiment)
```
Both lexicons have more negative than positive words, but the ratio of negative to positive words is higher in the Bing lexicon than the NRC lexicon. This will contribute to the effect we see in the plot above, as will any systematic difference in word matches, e.g. if the negative words in the NRC lexicon do not match the words that Jane Austen uses very well. Whatever the source of these differences, we see similar relative trajectories across the narrative arc, with similar changes in slope, but marked differences in absolute sentiment from lexicon to lexicon. This is all important context to keep in mind when choosing a sentiment lexicon for analysis.


```{r}
library(wordcloud)
cleandata3 %>%
        count(word) %>%
        with(wordcloud(word, n, max.words = 100))
```

```{r}
nrc_disgust <- get_sentiments("nrc") %>% 
  filter(sentiment == "disgust")


cleandata3 %>%
  inner_join(nrc_disgust) %>%
  count(word, sort = TRUE)

```

```{r}
nrc_anger <- get_sentiments("nrc") %>% 
  filter(sentiment == "anger")

cleandata3 %>%
  inner_join(nrc_anger) %>%
  count(word, sort = TRUE)
```

```{r}
nrc_fear <- get_sentiments("nrc") %>% 
  filter(sentiment == "fear")

cleandata3 %>%
  inner_join(nrc_fear) %>%
  count(word, sort = TRUE)
```
```{r}
nrc_surprise <- get_sentiments("nrc") %>% 
  filter(sentiment == "surprise")

cleandata3 %>%
  inner_join(nrc_surprise) %>%
  count(word, sort = TRUE)
```

```{r}
nrc_trust <- get_sentiments("nrc") %>% 
  filter(sentiment == "trust")

cleandata3 %>%
  inner_join(nrc_trust) %>%
  count(word, sort = TRUE)
```
```{r}
nrc_anticipation <- get_sentiments("nrc") %>% 
  filter(sentiment == "anticipation")

cleandata3 %>%
  inner_join(nrc_anticipation) %>%
  count(word, sort = TRUE)
```

```{r}
nrc_sadness <- get_sentiments("nrc") %>% 
  filter(sentiment == "sadness")

cleandata3 %>%
  inner_join(nrc_sadness) %>%
  count(word, sort = TRUE)
```

```{r}
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

cleandata3 %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```


```{r}
nrc_pn <- get_sentiments("nrc") %>% 
   filter(sentiment %in% c("positive", "negative"))

cleandata3 %>%
  inner_join(nrc_pn) %>%
  count(sentiment)
```

